* 博客园地址: http://www.cnblogs.com/jiangxinnju
* GitHub地址: https://github.com/jiangxincode
* 知乎地址: https://www.zhihu.com/people/jiangxinnju

# IKAnalyzer

http://code.google.com/p/ik-analyzer/

IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始，IKAnalyzer已经推出了3个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IKAnalyzer3.0则发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。

语言和平台： 基于java 语言开发，最初它是以开源项目Luence 为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IKAnalyzer 3.0 则发展为面向 Java 的公用分词组件，独立于 Lucene 项目，同时提供了对 Lucene 的默认优化实现。

算法：采用了特有的“正向迭代最细粒度切分算法” 。采用了多子处理器分析模式，支持：英文字母（ IP 地址、Email、URL ）、数字（日期、常用中文数量词、罗马数字、科学计数法），中文词汇（姓名、地名处理）等分词处理。优化的词典存储，更小的内存占用。支持用户词典扩展定义。针对 Lucene 全文检索优化的查询分析器 IKQueryParser ；采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高 Lucene 检索的命中率。

性能：60 万字 / 秒

IKAnalyzer基于lucene2.0版本API开发，实现了以词典分词为基础的正反向全切分算法，是LuceneAnalyzer接口的实现。该算法适合与互联网用户的搜索习惯和企业知识库检索，用户可以用句子中涵盖的中文词汇搜索，如用"人民"搜索含"人民币"的文章，这是大部分用户的搜索思维；不适合用于知识挖掘和网络爬虫技术，全切分法容易造成知识歧义，因为在语义学上"人民"和"人民币"是完全搭不上关系的。

# je-anlysis

分词效率： 每秒30万字（测试环境迅驰1.6，第一次分词需要1－2秒加载词典）

运行环境： Lucene 2.0，基于java实现。

免费安装使用传播，无限制商业应用，但暂不开源，也不提供任何保证

优点:全面支持Lucene 2.0；增强了词典维护的API；增加了商品编码的匹配；增加了Mail地址的匹配；实现了词尾消歧算法第二层的过滤；整理优化了词库；

支持词典的动态扩展；支持中文数字的匹配（如：二零零六）；数量词采用“n”；作为数字通配符优化词典结构以便修改调整；支持英文、数字、中文（简体）混合分词；常用的数量和人名的匹配；超过22万词的词库整理；实现正向最大匹配算法；支持分词粒度控制

# ICTCLAS

http://www.ictclas.org/index.html

# ictclas4j

ictclas4j中文分词系统是sinboy在中科院张华平和刘群老师的研制的FreeICTCLAS的基础上完成的一个java开源分词项目，简化了原分词程序的复杂度，旨在为广大的中文分词爱好者一个更好的学习机会。

性能：分词速度单机996KB/s ， API 不超过 200KB ，各种词典数据压缩后不到 3M.

准确率：分词精度98.45%

语言和平台：ICTCLAS 全部采用 C/C++ 编写，支持 Linux 、 FreeBSD 及 Windows 系列操作系统，支持 C/C++ 、 C# 、 Delphi 、 Java 等主流的开发语言。

Author：中国科学院计算技术研究所

主要功能：中文分词；词性标注；命名实体识别；新词识别；未登录词识别;同时支持用户词典；支持繁体中文；支持GBK 、 UTF-8 、 UTF-7 、 UNICODE 等多种编码格式。

算法：完美PDAT 大规模知识库管理技术（ 200510130690.3 ），在高速度与高精度之间取得了重大突破，该技术可以管理百万级别的词典知识库，单机每秒可以查询 100 万词条，而内存消耗不到知识库大小的 1.5 倍。层叠隐马尔可夫模型（ Hierarchical Hidden Markov Model ） ，该分词系统的主要是思想是先通过 CHMM( 层叠形马尔可夫模型 ) 进行分词 , 通过分层 , 既增加了分词的准确性 , 又保证了分词的效率 . 共分五层 , 如下图所示。基本思路是进行原子切分 , 然后在此基础上进行 N- 最短路径粗切分 , 找出前 N 个最符合的切分结果 , 生成二元分词表 , 然后生成分词结果 , 接着进行词性标注并完成主要分词步骤 .

# imdict

imdict-chinese-analyzer是imdict智能词典的智能中文分词模块，算法基于隐马尔科夫模型(Hidden Markov Model，HMM)，是中国科学院计算技术研究所的ictclas中文分词程序的重新实现（基于Java），可以直接为lucene搜索引擎提供简体中文分词支持。

imdict-chinese-analyzer 是imdict智能词典的智能中文分词模块

算法：基于隐马尔科夫模型(Hidden Markov Model，HMM)，是中国科学院计算技术研究所的 ictclas 中文分词程序的重新实现（基于Java），可以直接为lucene 搜索引擎提供简体中文分词支持 。

主要功能：

完全 Unicode 支持：分词核心模块完全采用Unicode 编码，无须各种汉字编码的转换，极大的提升了分词的效率。

提升搜索效率：根据imdict智能词典的实践，在有智能中文分词的情况下，索引文件比没有中文分词的索引文件小 1/3

提高搜索准确度：imdict -chinese-analyzer采用了 HHMM 分词模型，极大的提高了分词的准确率，在此基础上的搜索，比对汉字逐个切分要准确得多！

更高效的数据结构：为了提高效率，针对常用中文检索的应用场景，imdict-chinese-analyzer 对一些不必要的功能进行了删减，例如词性标注、人名识别、时间识别等等。另外还修改了算法的数据结构，在内存占用量缩减到 1/3 的情况下把效率提升了数倍。

# paoding

http://www.oschina.net/p/paoding/

Paoding's Knives中文分词基于Java的开源中文分词组件，提供lucene和solr 接口，具有极高效率和高扩展性。引入隐喻，采用完全的面向对象设计，构思先进。高效率：在PIII 1G内存个人机器上，1秒可准确分词100万汉字。采用基于不限制个数的词典文件对文章进行有效切分，使能够将对词汇分类定义。能够对未知的词汇进行合理解析。

语言和平台：Java  提供 lucence  3.0  接口，仅支持 Java 语言。 Paoding（庖丁解牛分词）基于Java的开源中文分词组件，提供lucene和solr 接口，具有极高效率和高扩展性 。引入隐喻，采用完全的面向对象设计，构思先进。

# MMSEG4J

基于Java的开源中文分词组件，提供lucene和solr 接口。

mmseg4j 用 Chih-Hao Tsai 的 MMSeg 算法实现的中文分词器，并实现 lucene 的 analyzer 和 solr 的TokenizerFactory 以方便在Lucene和Solr中使用。

MMSeg 算法有两种分词方法：Simple和Complex，都是基于正向最大匹配。Complex 加了四个规则过虑。官方说：词语的正确识别率达到了98.41% ，mmseg4j 已经实现了这两种分词算法。